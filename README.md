# ğŸ“˜ README â€” listen_client_widget

## ğŸ¤ Overview
**listen_client_widget** is a fully hands-free, browser-based AI voice assistant that connects to:

- **OpenAI Whisper** â†’ Speech-to-Text  
- **MCP Orchestrator** â†’ Stateful intent management (food ordering flow, memory, state)  
- **ElevenLabs** â†’ Natural Text-to-Speech  
- **Advanced Voice Activity Detection (VAD)** â†’ User-only speech detection  
- **Strictness B Mode** â†’ High-precision filtering  
- **Mode 2** â†’ Smart fallback if user voice shifts  
- **iOS-safe audio pipeline**  

This widget can be embedded into **any website**, **mobile browser**, or **web app** using a single `<script>` tag.

The backend is powered by **Flask**, designed specifically for **Railway deployment**, and acts as a â€œbridgeâ€ to:

- Whisper STT  
- MCP Orchestrator  
- ElevenLabs TTS

---

## ğŸ§© Project Structure

```
listen_client_widget/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                # Flask backend: STT â†’ MCP â†’ TTS pipeline
â”‚   â”œâ”€â”€ requirements.txt       # Python dependencies
â”‚   â”œâ”€â”€ Procfile               # Railway process file
â”‚   â”œâ”€â”€ .env.example           # Environment variable template
â”‚   â”‚
â”‚   â”œâ”€â”€ templates/
â”‚   â”‚    â””â”€â”€ index.html        # Simple test page with embedded widget
â”‚   â”‚
â”‚   â””â”€â”€ static/
â”‚        â”œâ”€â”€ widget.js         # Full voice widget (Advanced VAD + Strict B + Mode 2)
â”‚        â””â”€â”€ widget.css        # Widget styling
```

---

## ğŸš€ Features

### âœ” Hands-Free Voice Interaction  
Automatically listens when the user speaks.  
Automatically stops after the conversation ends (`session_done`).

### âœ” Advanced VAD (Voice Activity Detection)
Prevents false triggers from:

- Background noise  
- Other people talking  
- Environmental sounds  

Includes:

- RMS energy analysis  
- Pitch detection (autocorrelation)  
- Strictness B gating  
- Mode 2 recovery logic  

### âœ” Calibration Wizard (Noise + Voice)
Before starting, the user performs:

1. **Noise Calibration** (2 seconds)  
2. **Voice Calibration** (2 seconds)

A profile is built:

```json
{
  "noiseFloor": ...,
  "voiceMean": ...,
  "pitchMin": ...,
  "pitchMax": ...
}
```

Used by VAD to detect *only the calibrated user*.

### âœ” Whisper â†’ MCP â†’ ElevenLabs Pipeline  
1. Microphone audio recorded in WAV (16 kHz)  
2. Uploaded to backend (`/api/voice`)  
3. Whisper transcribes  
4. MCP Orchestrator produces contextual reply  
5. ElevenLabs generates TTS  
6. Widget plays audio  
7. Session auto-stops if ordering completed

### âœ” Mobile Friendly + iOS Safari Safe  
Includes:

- Audio context unlock  
- Visibility change recovery  
- Safari autoplay constraints handling  
- Automatic resume of suspended contexts  

### âœ” Embeddable Anywhere  
Add this to your page:

```html
<script src="YOUR_BACKEND_URL/static/widget.js" data-backend-url="YOUR_BACKEND_URL"></script>
```

Widget appears as a floating assistant in the browser.

---

## âš™ï¸ Setup Instructions

### 1. Clone the repo

```
git clone <your-repo-url>
cd listen_client_widget/backend
```

---

## ğŸ“¦ Install dependencies

```
pip install -r requirements.txt
```

---

## ğŸ” Environment Variables

Copy `.env.example` â†’ `.env`:

```
OPENAI_API_KEY=sk-...
ORCHESTRATOR_URL=https://your-orchestrator-url/orchestrate
ELEVENLABS_API_KEY=...
ELEVENLABS_VOICE_ID=EXAVITQu4vr4xnSDxMaL
ELEVENLABS_MODEL_ID=eleven_multilingual_v2
```

Railway â†’ Variables tab â†’ copy same keys.

---

## â–¶ï¸ Run Locally

```
python app.py
```

Then visit:

```
http://localhost:5000/
```

---

## ğŸ›« Deploy to Railway

1. Create new Railway service  
2. Choose **Deploy from GitHub** or **Upload folder**  
3. Set root directory to:

```
listen_client_widget/backend
```

4. Add environment variables  
5. Railway auto-detects Python  
6. App runs on:

```
https://your-railway-app.up.railway.app/
```

---

## ğŸ—£ Embedding the Widget in Any Webpage

Add:

```html
<script
  src="https://your-railway-instance.up.railway.app/static/widget.js"
  data-backend-url="https://your-railway-instance.up.railway.app"
></script>
```

The voice assistant will appear automatically.

---

## ğŸ§ª Testing

### Test Whisper + MCP + TTS  
Use curl or Postman:

```
POST /api/voice
Content-Type: multipart/form-data
audio: <audio/wav>
```

### Test UI  
Visit `index.html` via `/` route.

---

## ğŸ§  Architecture

```
User Speech
    â†“ microphone (browser)
VAD (widget.js)
    â†“ 
WAV Buffer (16 kHz)
    â†“
Flask Backend
    â†“ Whisper STT
MCP Orchestrator (state, memory)
    â†“ AI reply text
ElevenLabs TTS
    â†“ base64 audio
Browser Plays Response
```

Hands-free continues until Orchestrator responds:

```json
{ "session_done": true }
```

Then widget stops listening.

---

## ğŸ§¹ Future Enhancements

- Wake word ("Hey Blink")  
- Real-time streaming Whisper  
- WebRTC noise suppression  
- Multi-user profiles  
- Conversation history UI  
- Diagnostics dashboard  

---

## â¤ï¸ Support  
If you want:

- NEW wake-word version  
- Auto language detection  
- Desktop app bundle  
- Mobile PWA version  
- Embeddable SDK version  

Just ask!
